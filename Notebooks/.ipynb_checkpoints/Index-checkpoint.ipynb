{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reinforcement Learning Using OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Introduction to RL (Reinforcement Learning)\n",
    "\n",
    "Reinforcement learning (RL) is an area of machine learning concerned with how software agents ought to take actions in an environment so as to maximize some notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\n",
    "\n",
    "It differs from supervised learning in that labelled input/output pairs need not be presented, and sub-optimal actions need not be explicitly corrected. Instead the focus is finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).\n",
    "\n",
    "The environment is typically formulated as a Markov decision process (MDP), as many reinforcement learning algorithms for this context utilize dynamic programming techniques. The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.\n",
    "\n",
    "**source**: https://en.wikipedia.org/wiki/Reinforcement_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments with OpenAI Gym\n",
    "\n",
    "Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or Theano.\n",
    "\n",
    "The gym library is a collection of test problems — environments — that you can use to work out your reinforcement learning algorithms. These environments have a shared interface, allowing you to write general algorithms.\n",
    "\n",
    "**source**: https://gym.openai.com/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learn by doing -- A reinforcement learning experiment using OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Gym environment\n",
    "\n",
    "To start using OpenAI Gym, one must install it and import it into a Python environment. One can either use a pre-made environment for testing, or build their own.\n",
    "\n",
    "For this experiment, I will be using the pre-built 'CartPole' environment. The state consists of the cart's position and velocity, and the pole's angle and velocity. The actions are applying a force of +1 or -1 to the cart. The goal is to keep the pole balanced for 500 game steps. The reward is 1 on each step for as long as the pole is balanced.\n",
    "\n",
    "We can view the action and observation space as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Shape: Discrete(2)\n",
      "Action Example: 1\n",
      "Observation Space Shape: Box(4,)\n",
      "Observation Example: [ 0.0132411   0.0352145  -0.028805   -0.01244701]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "observation = env.reset()\n",
    "\n",
    "print(f\"Action Space Shape: {env.action_space}\")\n",
    "print(f\"Action Example: {env.action_space.sample()}\")\n",
    "\n",
    "print(f\"Observation Space Shape: {env.observation_space}\")\n",
    "print(f\"Observation Example: {observation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also render the environment, until an episode completes, as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enables recording\n",
    "# import gym\n",
    "# from gym.wrappers import Monitor\n",
    "# env = gym.make(\"Acrobot-v1\")\n",
    "# env = Monitor(env, './video')\n",
    "\n",
    "env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "  env.render()\n",
    "  action = env.action_space.sample()\n",
    "  observation, reward, done, info = env.step(action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the prior code block, we take random actions and only utilize the `done` information to determine if a session has finished. You can either run the code to see the animation for youself, or view the video below.\n",
    "\n",
    "<video height=\"320\" controls src=\"assets/cartpole_random.mp4\" style=\"display: block;margin-left: auto;margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a smarter agent with reinforcement learning and tabular methods\n",
    "\n",
    "Tabular methods use arrays and tables to hold an approximate of the value functions. More simply, they store every combination of state/action pairs and hold a perceived value of that pair. This simplifies the learning process because tables are very quick to compute and can give decent results given the state and action spaces are small enough. One can also reduce the size of the state and action spaces to make the agent learn evey quicker due to a smaller table to fill (at the cost of a reduced accuracy). One idea that can be implemented, that I will not use in the Acrobot example, is to transform state values before binning them. For example, one could use a sigmoid function to spread the state values out so that the center distrubtion gets put into more bins while the outliers get grouped together.\n",
    "\n",
    "To apply a tabular method to this CartPole example, we simply need a table that holds every possible state/action pair. Due to the size of this table, I bin the state values into 20 buckets so that there are 2 * 4^20 values to compute. I also add a randomness factor to force my agent to explore more in the beginning then slowly exploit more as it learns. Feel free to edit the global variables (hyperparameters) to see what happens. This tabular method is also very random in how well it does with a small amount of sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Values and Imports\n",
    "\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "RENDER_STEPS = 250\n",
    "SESSIONS = 500\n",
    "\n",
    "BINS = 20\n",
    "REWARD_DISCOUNT = 0.98\n",
    "LEARNING_RATE_DECAY = 0.995\n",
    "LEARNING_RATE_MIN = 0.150\n",
    "EXPLORATION_RATE = 1.0\n",
    "EXPLORATION_RATE_DECAY = 0.975\n",
    "EXPLORTATION_RATE_MIN = 0.005\n",
    "\n",
    "BUFFER_SIZE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include a plotting class to show results :]\n",
    "\n",
    "class Plotter:\n",
    "    def __init__(self):\n",
    "        self._x_values = []\n",
    "        self._y_values = []\n",
    "\n",
    "    def add_plot_pair(self, x, y):\n",
    "        self._x_values.append(x)\n",
    "        self._y_values.append(y)\n",
    "\n",
    "    def plot(self):\n",
    "        plt.plot(self._x_values, self._y_values)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_and_save(self, file_name):\n",
    "        plt.plot(self._x_values, self._y_values)\n",
    "        plt.savefig(file_name)\n",
    "\n",
    "    def plot_smooth_graph(self, averaging_filter_length=100):\n",
    "        if averaging_filter_length > len(self._x_values):\n",
    "            self.plot()\n",
    "            return\n",
    "\n",
    "        rolling_average = []\n",
    "        for index in range(len(self._x_values)):\n",
    "            min_index = index - (averaging_filter_length / 2)\n",
    "            min_index = int(0 if min_index < 0 else min_index)\n",
    "            max_index = index + (averaging_filter_length / 2)\n",
    "            max_index = int(\n",
    "                len(self._x_values) - 1\n",
    "                if max_index >= len(self._x_values)\n",
    "                else max_index\n",
    "            )\n",
    "\n",
    "            rolling_average.append(\n",
    "                sum(self._y_values[min_index:max_index]) // (max_index - min_index)\n",
    "            )\n",
    "\n",
    "        plt.title(\"Steps Achieved Per Session Over Training Duration\")\n",
    "        plt.xlabel(\"Sessions\")\n",
    "        plt.ylabel(\"Steps Achieved Per Session\")\n",
    "        plt.plot(\n",
    "            self._x_values,\n",
    "            rolling_average,\n",
    "            color=\"#ade6bb\",\n",
    "            linewidth=2.5,\n",
    "            label=f\"Running Average of {averaging_filter_length} Sessions\",\n",
    "        )\n",
    "        plt.plot(\n",
    "            self._x_values,\n",
    "            self._y_values,\n",
    "            color=\"#e6add8\",\n",
    "            linewidth=0.25,\n",
    "            label=f\"Raw Steps Achieved\",\n",
    "        )\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some selective replay experience\n",
    "\n",
    "class ReplayExperience:\n",
    "    def __init__(self, buffer_size):\n",
    "        self._buffer = []\n",
    "        self._buffer_size = buffer_size\n",
    "        self._forgetfullness = 0.15\n",
    "\n",
    "    def get_experience(self):\n",
    "        return random.choice(self._buffer)[1]\n",
    "\n",
    "    def make_experience(self, value, experience):\n",
    "        if len(self._buffer) < self._buffer_size:\n",
    "            self._buffer.append((value, experience))\n",
    "            return\n",
    "\n",
    "        if np.random.rand() <= self._forgetfullness:\n",
    "            del self._buffer[np.random.randint(len(self._buffer))]\n",
    "\n",
    "        # Find the worst memory and replace it\n",
    "        min_value = value\n",
    "        min_index = -1\n",
    "        for index in range(len(self._buffer)):\n",
    "            if self._buffer[index][0] < min_value:\n",
    "                min_value = self._buffer[index][0]\n",
    "                min_index = index\n",
    "        if min_index != -1:\n",
    "            self._buffer[min_index] = (value, experience)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main table class that includes data normalization using sigmoids\n",
    "\n",
    "class Table:\n",
    "    def __init__(\n",
    "        self,\n",
    "        action_space,\n",
    "        observation_space,\n",
    "        bins,\n",
    "        reward_dicount,\n",
    "        exploration_rate,\n",
    "        exploration_rate_min,\n",
    "        exploration_rate_decay,\n",
    "        learning_rate_min,\n",
    "        learning_rate_decay,\n",
    "    ):\n",
    "        self._action_space = action_space\n",
    "        self._observation_space = observation_space\n",
    "        self._bins = bins\n",
    "        self._reward_discount = reward_dicount\n",
    "        self._exploration_rate = exploration_rate\n",
    "        self._exploration_rate_min = exploration_rate_min\n",
    "        self._exploration_rate_decay = exploration_rate_decay\n",
    "        self._learning_rates = np.ones(\n",
    "            (bins ** observation_space.shape[0], action_space.n)\n",
    "        )\n",
    "        self._learning_rate_min = learning_rate_min\n",
    "        self._learning_rate_decay = learning_rate_decay\n",
    "\n",
    "        self._table = np.zeros((bins ** observation_space.shape[0], action_space.n))\n",
    "\n",
    "    def act(self, observation):\n",
    "        observation = self._scale_observation(observation)\n",
    "        bin = self._bin_observation(observation)\n",
    "\n",
    "        if np.random.rand() <= self._exploration_rate:\n",
    "            return self._action_space.sample()\n",
    "\n",
    "        return np.argmax(self._table[bin])\n",
    "\n",
    "    def learn_from_experience(self, experience):\n",
    "        penalty = -sum(step[2] for step in experience)\n",
    "        experience_length = len(experience)\n",
    "        discount = self._reward_discount\n",
    "\n",
    "        for index in range(experience_length - 1, 0, -1):\n",
    "            observation, action, _ = experience[index]\n",
    "            observation = self._scale_observation(observation)\n",
    "            bin = self._bin_observation(observation)\n",
    "\n",
    "            self._table[bin][action] += (\n",
    "                self._learning_rates[bin][action] * discount * penalty\n",
    "            )\n",
    "\n",
    "            discount *= self._reward_discount\n",
    "            self._update_learning_rate(bin, action)\n",
    "\n",
    "        self._update_exploration_rate()\n",
    "\n",
    "    def _update_learning_rate(self, bin, action):\n",
    "        self._learning_rates[bin][action] *= self._learning_rate_decay\n",
    "        self._learning_rates[bin][action] = max(\n",
    "            self._learning_rate_min, self._learning_rates[bin][action]\n",
    "        )\n",
    "\n",
    "    def _update_exploration_rate(self):\n",
    "        self._exploration_rate *= self._exploration_rate_decay\n",
    "        self._exploration_rate = max(self._exploration_rate_min, self._exploration_rate)\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def _scale_observation(self, observation):\n",
    "        scaled_observation = []\n",
    "\n",
    "        for index in range(len(self._observation_space.low)):\n",
    "            high = self._sigmoid(self._observation_space.high[index])\n",
    "            low = self._sigmoid(self._observation_space.low[index])\n",
    "\n",
    "            scaled_value = self._sigmoid(observation[index])\n",
    "            scaled_value = (scaled_value - low) / (high - low)\n",
    "            normalized_value = scaled_value * self._bins\n",
    "\n",
    "            scaled_observation.append(int(normalized_value))\n",
    "\n",
    "        return scaled_observation\n",
    "\n",
    "    def _bin_observation(self, observation):\n",
    "        bin = 0\n",
    "        for index in range(len(observation)):\n",
    "            bin += (2 ** index) * observation[index]\n",
    "        return int(bin)  # must be an int since it is used as an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating objects before training starts\n",
    "\n",
    "plotter = Plotter()\n",
    "replay_experience = ReplayExperience(BUFFER_SIZE)\n",
    "table = Table(\n",
    "    env.action_space,\n",
    "    env.observation_space,\n",
    "    BINS,\n",
    "    REWARD_DISCOUNT,\n",
    "    EXPLORATION_RATE,\n",
    "    EXPLORTATION_RATE_MIN,\n",
    "    EXPLORATION_RATE_DECAY,\n",
    "    LEARNING_RATE_MIN,\n",
    "    LEARNING_RATE_DECAY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "for session in range(1, SESSIONS + 1):  # offset by 1 for better formatting in results\n",
    "    done = False\n",
    "    step = 0\n",
    "    experience = []\n",
    "\n",
    "    observation = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        if session % RENDER_STEPS == 1:\n",
    "            env.render()\n",
    "        step += 1\n",
    "\n",
    "        action = table.act(observation)\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "\n",
    "        experience.append((observation, action, reward))\n",
    "\n",
    "    table.learn_from_experience(experience)\n",
    "    replay_experience.make_experience(step, experience)\n",
    "    experience = replay_experience.get_experience()\n",
    "    table.learn_from_experience(experience)\n",
    "\n",
    "    # print(f\"Session {session} took {step} steps.\")  -- prints too much in Jupyter\n",
    "    plotter.add_plot_pair(session, step)\n",
    "\n",
    "env.close()\n",
    "plotter.plot_smooth_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, if you do not wish to run the code, some results are shown below. Unfortunatly, the graph does not show the legend; the green line shows a running average of 100 sessions while the light pink line shows the actual values at each session.\n",
    "\n",
    "<img height=\"320\" src=\"assets/cartpole_figure.png\" style=\"display: block;margin-left: auto;margin-right: auto;\"/>\n",
    "\n",
    "<video height=\"320\" controls src=\"assets/cartpole_trained.mp4\" style=\"display: block;margin-left: auto;margin-right: auto;\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
